{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intelligent Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YltVFV-a8moo",
        "colab_type": "code",
        "outputId": "4eeb29bd-6071-4e2a-db50-b30266a37a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "!pip3 install tensorflow==1.0.0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/58/b71480f9ec9d08d581d672a81b15ab5fec36a5fcda2093558a23614d8468/tensorflow-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (44.5MB)\n",
            "\u001b[K     |████████████████████████████████| 44.5MB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.16.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.0) (41.0.1)\n",
            "\u001b[31mERROR: stable-baselines 2.2.1 has requirement tensorflow>=1.5.0, but you'll have tensorflow 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed tensorflow-1.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Z_UxaziMle",
        "colab_type": "code",
        "outputId": "e71fd43b-9971-4df5-ce8b-c4e59d3deaaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "!apt-get remove cuda\n",
        "!apt-get autoremove cuda\n",
        "!apt-get purge cuda\n",
        "!apt-key del /var/cuda-repo-9-2-local/*.pub\n",
        "!rm -rf /var/cuda-repo-8-0-local-ga2/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "The following packages will be REMOVED:\n",
            "  libnvidia-common-410\n",
            "0 upgraded, 0 newly installed, 1 to remove and 8 not upgraded.\n",
            "After this operation, 33.8 kB disk space will be freed.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Removing libnvidia-common-410 (410.104-0ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af6XNL3UiN-V",
        "colab_type": "code",
        "outputId": "73e55675-6a05-4e0f-f667-d6fbebc2e68c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4333
        }
      },
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\n",
        "!dpkg -i --force-overwrite cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\n",
        "!apt-get update\n",
        "!apt-get install cuda-8-0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-07 06:06:33--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.211.70, 2606:2800:21f:3aa:dcf:37b:1ed6:1fb\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.211.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2690 (2.6K) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604_8.0.61-1_amd64.deb’\n",
            "\n",
            "\r          cuda-repo   0%[                    ]       0  --.-KB/s               \rcuda-repo-ubuntu160 100%[===================>]   2.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-06-07 06:06:33 (145 MB/s) - ‘cuda-repo-ubuntu1604_8.0.61-1_amd64.deb’ saved [2690/2690]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604.\n",
            "(Reading database ... 130907 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu1604 (8.0.61-1) ...\n",
            "Setting up cuda-repo-ubuntu1604 (8.0.61-1) ...\n",
            "\n",
            "Configuration file '/etc/apt/sources.list.d/cuda.list'\n",
            " ==> File on system created by you or by a script.\n",
            " ==> File also in package provided by package maintainer.\n",
            "   What would you like to do about it ?  Your options are:\n",
            "    Y or I  : install the package maintainer's version\n",
            "    N or O  : keep your currently-installed version\n",
            "      D     : show the differences between the versions\n",
            "      Z     : start a shell to examine the situation\n",
            " The default action is to keep your current version.\n",
            "*** cuda.list (Y/I/N/O/D/Z) [default=N] ? Y\n",
            "Installing new version of config file /etc/apt/sources.list.d/cuda.list ...\n",
            "Warning: The postinst maintainerscript of the package cuda-repo-ubuntu1604\n",
            "Warning: seems to use apt-key (provided by apt) without depending on gnupg or gnupg2.\n",
            "Warning: This will BREAK in the future and should be fixed by the package maintainer(s).\n",
            "Note: Check first if apt-key functionality is needed at all - it probably isn't!\n",
            "Warning: apt-key should not be used in scripts (called from postinst maintainerscript of the package cuda-repo-ubuntu1604)\n",
            "OK\n",
            "Ign:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
            "Get:2 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release [564 B]\n",
            "Get:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release.gpg [801 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages [209 kB]\n",
            "Get:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:13 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [56.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,647 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [324 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [830 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [486 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,215 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3,902 B]\n",
            "Get:22 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [790 kB]\n",
            "Fetched 5,834 kB in 4s (1,446 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda-8-0 cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "0 upgraded, 31 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,352 MB of archives.\n",
            "After this operation, 2,144 MB of additional disk space will be used.\n",
            "Get:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-license-8-0 8.0.61-1 [27.6 kB]\n",
            "Get:2 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-misc-headers-8-0 8.0.61-1 [1,077 kB]\n",
            "Get:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-core-8-0 8.0.61-1 [20.0 MB]\n",
            "Get:4 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cudart-8-0 8.0.61-1 [135 kB]\n",
            "Get:5 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-driver-dev-8-0 8.0.61-1 [14.1 kB]\n",
            "Get:6 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cudart-dev-8-0 8.0.61-1 [1,071 kB]\n",
            "Get:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-command-line-tools-8-0 8.0.61-1 [26.1 MB]\n",
            "Get:8 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvrtc-8-0 8.0.61-1 [9,585 kB]\n",
            "Get:9 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvrtc-dev-8-0 8.0.61-1 [10.8 kB]\n",
            "Get:10 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cusolver-8-0 8.0.61-1 [29.3 MB]\n",
            "Get:11 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cusolver-dev-8-0 8.0.61-1 [6,816 kB]\n",
            "Get:12 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cublas-8-0 8.0.61.2-1 [58.1 MB]\n",
            "Get:13 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cublas-dev-8-0 8.0.61.2-1 [66.6 MB]\n",
            "Get:14 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cufft-8-0 8.0.61-1 [117 MB]\n",
            "Get:15 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cufft-dev-8-0 8.0.61-1 [94.8 MB]\n",
            "Get:16 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-curand-8-0 8.0.61-1 [43.7 MB]\n",
            "Get:17 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-curand-dev-8-0 8.0.61-1 [67.7 MB]\n",
            "Get:18 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cusparse-8-0 8.0.61-1 [28.8 MB]\n",
            "Get:19 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cusparse-dev-8-0 8.0.61-1 [29.6 MB]\n",
            "Get:20 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-npp-8-0 8.0.61-1 [157 MB]\n",
            "Get:21 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-npp-dev-8-0 8.0.61-1 [82.3 MB]\n",
            "Get:22 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-samples-8-0 8.0.61-1 [101 MB]\n",
            "Get:23 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-documentation-8-0 8.0.61-1 [113 MB]\n",
            "Get:24 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvml-dev-8-0 8.0.61-1 [48.4 kB]\n",
            "Get:25 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvgraph-8-0 8.0.61-1 [2,948 kB]\n",
            "Get:26 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvgraph-dev-8-0 8.0.61-1 [3,028 kB]\n",
            "Get:27 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-visual-tools-8-0 8.0.61-1 [286 MB]\n",
            "Get:28 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-toolkit-8-0 8.0.61-1 [2,892 B]\n",
            "Get:29 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-runtime-8-0 8.0.61-1 [2,574 B]\n",
            "Get:30 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-demo-suite-8-0 8.0.61-1 [4,988 kB]\n",
            "Get:31 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-8-0 8.0.61-1 [2,556 B]\n",
            "Fetched 1,352 MB in 25s (54.5 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-license-8-0.\n",
            "(Reading database ... 130910 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-license-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-8-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-core-8-0.\n",
            "Preparing to unpack .../02-cuda-core-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-core-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-8-0.\n",
            "Preparing to unpack .../03-cuda-cudart-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-8-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-8-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-8-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-8-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-8-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-8-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-8-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cublas-8-0.\n",
            "Preparing to unpack .../11-cuda-cublas-8-0_8.0.61.2-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-8-0 (8.0.61.2-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-8-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-8-0_8.0.61.2-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-8-0 (8.0.61.2-1) ...\n",
            "Selecting previously unselected package cuda-cufft-8-0.\n",
            "Preparing to unpack .../13-cuda-cufft-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-8-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-8-0.\n",
            "Preparing to unpack .../15-cuda-curand-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-8-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-8-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-8-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-8-0.\n",
            "Preparing to unpack .../19-cuda-npp-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-8-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-samples-8-0.\n",
            "Preparing to unpack .../21-cuda-samples-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-samples-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-documentation-8-0.\n",
            "Preparing to unpack .../22-cuda-documentation-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-8-0.\n",
            "Preparing to unpack .../23-cuda-nvml-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-8-0.\n",
            "Preparing to unpack .../24-cuda-nvgraph-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-8-0.\n",
            "Preparing to unpack .../25-cuda-nvgraph-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-8-0.\n",
            "Preparing to unpack .../26-cuda-visual-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-8-0.\n",
            "Preparing to unpack .../27-cuda-toolkit-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-runtime-8-0.\n",
            "Preparing to unpack .../28-cuda-runtime-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-8-0.\n",
            "Preparing to unpack .../29-cuda-demo-suite-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-8-0.\n",
            "Preparing to unpack .../30-cuda-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-license-8-0 (8.0.61-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-8.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-8-0 (8.0.61.2-1) ...\n",
            "Setting up cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-core-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-dev-8-0 (8.0.61.2-1) ...\n",
            "Setting up cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-samples-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-8-0 (8.0.61-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_DHV0lIiN4l",
        "colab_type": "code",
        "outputId": "5df85969-1622-4d3a-eaca-ed6b6e7bd93c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!wget http://archive.ubuntu.com/ubuntu/pool/main/m/mesa/libglx-mesa0_18.0.5-0ubuntu0~18.04.1_amd64.deb\n",
        "!dpkg -i --force-overwrite libglx-mesa0_18.0.5-0ubuntu0~18.04.1_amd64.deb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-07 06:11:46--  http://archive.ubuntu.com/ubuntu/pool/main/m/mesa/libglx-mesa0_18.0.5-0ubuntu0~18.04.1_amd64.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 91.189.88.149, 91.189.88.152, 91.189.88.161, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|91.189.88.149|:80... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2019-06-07 06:11:47 ERROR 404: Not Found.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;31merror:\u001b[0m cannot access archive 'libglx-mesa0_18.0.5-0ubuntu0~18.04.1_amd64.deb': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdP59ZqYiNz1",
        "colab_type": "code",
        "outputId": "8aa00777-db4a-4dd0-abe6-22d937f023ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "!wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/nvidia-410_410.48-0ubuntu1_amd64.deb\n",
        "!dpkg -i --force-overwrite nvidia-410_410.48-0ubuntu1_amd64.deb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-07 06:11:59--  http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/nvidia-410_410.48-0ubuntu1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.211.70, 2606:2800:21f:3aa:dcf:37b:1ed6:1fb\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.211.70|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97367486 (93M) [application/x-deb]\n",
            "Saving to: ‘nvidia-410_410.48-0ubuntu1_amd64.deb’\n",
            "\n",
            "\r          nvidia-41   0%[                    ]       0  --.-KB/s               \r         nvidia-410  29%[====>               ]  27.37M   137MB/s               \r        nvidia-410_  80%[===============>    ]  74.44M   186MB/s               \rnvidia-410_410.48-0 100%[===================>]  92.86M   194MB/s    in 0.5s    \n",
            "\n",
            "2019-06-07 06:12:00 (194 MB/s) - ‘nvidia-410_410.48-0ubuntu1_amd64.deb’ saved [97367486/97367486]\n",
            "\n",
            "Selecting previously unselected package nvidia-410.\n",
            "\u001b[1mdpkg:\u001b[0m regarding nvidia-410_410.48-0ubuntu1_amd64.deb containing nvidia-410:\n",
            " nvidia-410 conflicts with nvidia-persistenced\n",
            "  nvidia-compute-utils-418 provides nvidia-persistenced and is present and installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing archive nvidia-410_410.48-0ubuntu1_amd64.deb (--install):\n",
            " conflicting packages - not installing nvidia-410\n",
            "Errors were encountered while processing:\n",
            " nvidia-410_410.48-0ubuntu1_amd64.deb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv-fcJdyiNvZ",
        "colab_type": "code",
        "outputId": "1a80c91d-8a4f-415c-9575-64e204be4aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!apt --fix-broken install\n",
        "!apt-get install cuda-8-0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cuda-8-0 is already the newest version (8.0.61-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZLoKwxziNqE",
        "colab_type": "code",
        "outputId": "d8e7b344-79ac-4801-c274-b4d326c31f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2016 NVIDIA Corporation\n",
            "Built on Tue_Jan_10_13:22:03_CST_2017\n",
            "Cuda compilation tools, release 8.0, V8.0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUixmDawidJw",
        "colab_type": "code",
        "outputId": "623ef3c6-b960-4b39-a3b3-e42d2f3bbbd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/e2/7389e7a1c10eb209ddabe0b35cca2e522a3985a1df7e07904c76d1d5609c/tensorflow_gpu-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (95.3MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 62.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow-gpu==1.0) (41.0.1)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPJASfCjiNg_",
        "colab_type": "code",
        "outputId": "2c4fa6b7-6d14-4b86-ae42-b1698041ffff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzabd6nzii6F",
        "colab_type": "code",
        "outputId": "df56e88c-0e76-4058-8ac7-4c28edecfa85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAroWP6D8u5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building a ChatBot with Deep NLP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af41VYtw8uXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpR9y6LKhl-T",
        "colab_type": "code",
        "outputId": "5dd94167-e9a6-4198-db76-72cdbc3b0645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CK2v4oy81Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## PART 1 - DATA PREPROCESSING ##########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3wLnsyB84WI",
        "colab_type": "code",
        "outputId": "c5267aad-1156-48df-fb27-735242c9c5b4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cb721e4c-bf28-473a-a531-02b51b7e8234\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-cb721e4c-bf28-473a-a531-02b51b7e8234\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving movie_lines.txt to movie_lines (2).txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwSKio5F8_wR",
        "colab_type": "code",
        "outputId": "fe2d2660-e12c-410e-9e26-f4e9117c3f87",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a3f428fa-1b9e-4e90-827c-1107db88e407\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a3f428fa-1b9e-4e90-827c-1107db88e407\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving movie_conversations.txt to movie_conversations.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7xr2QOy9EtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the dataset\n",
        "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV27Ik0P9IHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary that maps each line and its id\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4l4CATZ9NQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a list of all of the conversations\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D73oVuVd9QMB",
        "colab_type": "code",
        "outputId": "7710498e-75c8-4558-8748-4c2b8043f5c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "# Getting separately the questions and the answers\n",
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "  for i in range(len(conversation) - 1):\n",
        "    questions.append(id2line[conversation[i]])\n",
        "    answers.append(id2line[conversation[i+1]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-dd94c68f3a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconversation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversations_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mquestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'L650293'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8YXbQpb9T25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Doing a first cleaning of the texts\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttfi8w-y9YdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the questions\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7PqOdRn9auh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the answers\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vZ6NB0-9dc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filtering out the questions and answers that are too short or too long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if 2 <= len(question.split()) <= 25:\n",
        "        short_questions.append(question)\n",
        "        short_answers.append(clean_answers[i])\n",
        "    i += 1\n",
        "clean_questions = []\n",
        "clean_answers = []\n",
        "i = 0\n",
        "for answer in short_answers:\n",
        "    if 2 <= len(answer.split()) <= 25:\n",
        "        clean_answers.append(answer)\n",
        "        clean_questions.append(short_questions[i])\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-zjILbX9hIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary that maps each word to its number of occurrences\n",
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPYGewDS9kM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
        "threshold_questions = 15\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "threshold_answers = 15\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxo4DFEv9nug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding the last tokens to these two dictionaries\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) + 1\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF-BJbqG9ql4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the inverse dictionary of the answerswords2int dictionary\n",
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNFRlPOh9tHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding the End Of String token to the end of every answer\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += ' <EOS>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG2CvlsO9wIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Translating all the questions and the answers into integers\n",
        "# and Replacing all the words that were filtered out by <OUT> \n",
        "questions_into_int = []\n",
        "for question in clean_questions:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "answers_into_int = []\n",
        "for answer in clean_answers:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGiEks8Y9zPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sorting questions and answers by the length of questions\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rZgls-q916R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## PART 2 - BUILDING THE SEQ2SEQ MODEL ##########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOuYfVSN94L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating placeholders for the inputs and the targets\n",
        "def model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
        "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
        "    return inputs, targets, lr, keep_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu_PsA8D96t5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing the targets\n",
        "def preprocess_targets(targets, word2int, batch_size):\n",
        "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
        "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
        "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
        "    return preprocessed_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhLgwvt799YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Encoder RNN\n",
        "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
        "                                                                    cell_bw = encoder_cell,\n",
        "                                                                    sequence_length = sequence_length,\n",
        "                                                                    inputs = rnn_inputs,\n",
        "                                                                    dtype = tf.float32)\n",
        "    return encoder_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q14tRcKw-Ag3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoding the training set\n",
        "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              name = \"attn_dec_train\")\n",
        "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                              training_decoder_function,\n",
        "                                                                                                              decoder_embedded_input,\n",
        "                                                                                                              sequence_length,\n",
        "                                                                                                              scope = decoding_scope)\n",
        "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
        "    return output_function(decoder_output_dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfNRzJut-Dw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoding the test/validation set\n",
        "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
        "                                                                              encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              decoder_embeddings_matrix,\n",
        "                                                                              sos_id,\n",
        "                                                                              eos_id,\n",
        "                                                                              maximum_length,\n",
        "                                                                              num_words,\n",
        "                                                                              name = \"attn_dec_inf\")\n",
        "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                                test_decoder_function,\n",
        "                                                                                                                scope = decoding_scope)\n",
        "    return test_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTKvQM3C-IUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Decoder RNN\n",
        "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
        "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
        "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
        "        biases = tf.zeros_initializer()\n",
        "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
        "                                                                      num_words,\n",
        "                                                                      None,\n",
        "                                                                      scope = decoding_scope,\n",
        "                                                                      weights_initializer = weights,\n",
        "                                                                      biases_initializer = biases)\n",
        "        training_predictions = decode_training_set(encoder_state,\n",
        "                                                   decoder_cell,\n",
        "                                                   decoder_embedded_input,\n",
        "                                                   sequence_length,\n",
        "                                                   decoding_scope,\n",
        "                                                   output_function,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size)\n",
        "        decoding_scope.reuse_variables()\n",
        "        test_predictions = decode_test_set(encoder_state,\n",
        "                                           decoder_cell,\n",
        "                                           decoder_embeddings_matrix,\n",
        "                                           word2int['<SOS>'],\n",
        "                                           word2int['<EOS>'],\n",
        "                                           sequence_length - 1,\n",
        "                                           num_words,\n",
        "                                           decoding_scope,\n",
        "                                           output_function,\n",
        "                                           keep_prob,\n",
        "                                           batch_size)\n",
        "    return training_predictions, test_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riVj79W7-NCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building the seq2seq model\n",
        "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
        "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
        "                                                              answers_num_words + 1,\n",
        "                                                              encoder_embedding_size,\n",
        "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
        "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
        "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
        "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
        "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
        "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
        "                                                         decoder_embeddings_matrix,\n",
        "                                                         encoder_state,\n",
        "                                                         questions_num_words,\n",
        "                                                         sequence_length,\n",
        "                                                         rnn_size,\n",
        "                                                         num_layers,\n",
        "                                                         questionswords2int,\n",
        "                                                         keep_prob,\n",
        "                                                         batch_size)\n",
        "    return training_predictions, test_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-pWSus4-d9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## PART 3 - TRAINING THE SEQ2SEQ MODEL ##########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C_IL5j2-iFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "rnn_size = 1024\n",
        "num_layers = 3\n",
        "encoding_embedding_size = 1024\n",
        "decoding_embedding_size = 1024\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0.9\n",
        "min_learning_rate = 0.0001\n",
        "keep_probability = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW17TQQj-kly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a session\n",
        "tf.reset_default_graph()\n",
        "session = tf.InteractiveSession()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azcIfnrs-nHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model inputs\n",
        "inputs, targets, lr, keep_prob = model_inputs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvoREho_-pj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the sequence length\n",
        "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bWwdJtY-sps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the shape of the inputs tensor\n",
        "input_shape = tf.shape(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyvvlhjt-xJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the training and test predictions\n",
        "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                       targets,\n",
        "                                                       keep_prob,\n",
        "                                                       batch_size,\n",
        "                                                       sequence_length,\n",
        "                                                       len(answerswords2int),\n",
        "                                                       len(questionswords2int),\n",
        "                                                       encoding_embedding_size,\n",
        "                                                       decoding_embedding_size,\n",
        "                                                       rnn_size,\n",
        "                                                       num_layers,\n",
        "                                                       questionswords2int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdG6tfAR-0na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up the Loss Error, the Optimizer and Gradient Clipping\n",
        "with tf.name_scope(\"optimization\"):\n",
        "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
        "                                                  targets,\n",
        "                                                  tf.ones([input_shape[0], sequence_length]))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    gradients = optimizer.compute_gradients(loss_error)\n",
        "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
        "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JaaUxnl-5pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding the sequences with the <PAD> token\n",
        "def apply_padding(batch_of_sequences, word2int):\n",
        "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCzThe4r_A2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the data into batches of questions and answers\n",
        "def split_into_batches(questions, answers, batch_size):\n",
        "    for batch_index in range(0, len(questions) // batch_size):\n",
        "        start_index = batch_index * batch_size\n",
        "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
        "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
        "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
        "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
        "        yield padded_questions_in_batch, padded_answers_in_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03zcpVJ-_FKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the questions and answers into training and validation sets\n",
        "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
        "training_questions = sorted_clean_questions[training_validation_split:]\n",
        "training_answers = sorted_clean_answers[training_validation_split:]\n",
        "validation_questions = sorted_clean_questions[:training_validation_split]\n",
        "validation_answers = sorted_clean_answers[:training_validation_split]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muaepzk3_Hw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "batch_index_check_training_loss = 100\n",
        "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
        "total_training_loss_error = 0\n",
        "list_validation_loss_error = []\n",
        "early_stopping_check = 0\n",
        "early_stopping_stop = 100\n",
        "checkpoint = \"chatbot_weights.ckpt\"\n",
        "session.run(tf.global_variables_initializer())\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
        "        starting_time = time.time()\n",
        "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
        "                                                                                               targets: padded_answers_in_batch,\n",
        "                                                                                               lr: learning_rate,\n",
        "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                                               keep_prob: keep_probability})\n",
        "        total_training_loss_error += batch_training_loss_error\n",
        "        ending_time = time.time()\n",
        "        batch_time = ending_time - starting_time\n",
        "        if batch_index % batch_index_check_training_loss == 0:\n",
        "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
        "                                                                                                                                       epochs,\n",
        "                                                                                                                                       batch_index,\n",
        "                                                                                                                                       len(training_questions) // batch_size,\n",
        "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
        "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
        "            total_training_loss_error = 0\n",
        "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
        "            total_validation_loss_error = 0\n",
        "            starting_time = time.time()\n",
        "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
        "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
        "                                                                       targets: padded_answers_in_batch,\n",
        "                                                                       lr: learning_rate,\n",
        "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                       keep_prob: 1})\n",
        "                total_validation_loss_error += batch_validation_loss_error\n",
        "            ending_time = time.time()\n",
        "            batch_time = ending_time - starting_time\n",
        "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
        "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
        "            learning_rate *= learning_rate_decay\n",
        "            if learning_rate < min_learning_rate:\n",
        "                learning_rate = min_learning_rate\n",
        "            list_validation_loss_error.append(average_validation_loss_error)\n",
        "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
        "                print('I speak better now!!')\n",
        "                early_stopping_check = 0\n",
        "                saver = tf.train.Saver()\n",
        "                saver.save(session, checkpoint)\n",
        "            else:\n",
        "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
        "                early_stopping_check += 1\n",
        "                if early_stopping_check == early_stopping_stop:\n",
        "                    break\n",
        "    if early_stopping_check == early_stopping_stop:\n",
        "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
        "        break\n",
        "print(\"Game Over\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA5OTv2G_Npr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## PART 4 - TESTING THE SEQ2SEQ MODEL ##########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21HvBqSr_QPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the weights and Running the session\n",
        "checkpoint = \"./chatbot_weights.ckpt\"\n",
        "session = tf.InteractiveSession()\n",
        "session.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(session, checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqM9T74Q_TIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the questions from strings to lists of encoding integers\n",
        "def convert_string2int(question, word2int):\n",
        "    question = clean_text(question)\n",
        "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_G2THm7_Vp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up the chat\n",
        "while(True):\n",
        "    question = input(\"You: \")\n",
        "    if question == 'Goodbye':\n",
        "        break\n",
        "    question = convert_string2int(question, questionswords2int)\n",
        "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
        "    fake_batch = np.zeros((batch_size, 25))\n",
        "    fake_batch[0] = question\n",
        "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
        "    answer = ''\n",
        "    for i in np.argmax(predicted_answer, 1):\n",
        "        if answersints2word[i] == 'i':\n",
        "            token = ' I'\n",
        "        elif answersints2word[i] == '<EOS>':\n",
        "            token = '.'\n",
        "        elif answersints2word[i] == '<OUT>':\n",
        "            token = 'out'\n",
        "        else:\n",
        "            token = ' ' + answersints2word[i]\n",
        "        answer += token\n",
        "        if token == '.':\n",
        "            break\n",
        "    print('ChatBot: ' + answer)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}